function [Wout,Vout, trainingError, testError ] = trainMultiLayer(Xtraining,Dtraining,Xtest,Dtest, W0, V0,numIterations, learningRate )
%TRAINMULTILAYER Trains the network (Learning)
%   Inputs:
%               X* - Trainin/test features (matrix)
%               D* - Training/test desired output of net (matrix)
%               V0 - Weights of the output neurons (matrix)
%               W0 - Weights of the output neurons (matrix)
%
%   Output:
%               Y = Output for each feature, (matrix)
%               L = The resulting label of each feature, (vector) 

% Initiate variables
trainingError = nan(numIterations+1,1);
testError = nan(numIterations+1,1);
numTraining = size(Xtraining,2);
numTest = size(Xtest,2);
numClasses = u
Wout = W0;
Vout = V0;

% Calculate initial error
Ytraining = runMultiLayer(Xtraining, W0, V0);
Ytest = runMultiLayer(Xtest, W0, V0);
trainingError(1) = sum(sum((Ytraining - Dtraining).^2))/numTraining;
testError(1) = sum(sum((Ytest - Dtest).^2))/numTest;

for n = 1:numIterations
Ytraining = runMultiLayer(Xtraining, Wout, Vout);

grad_v = 2/numTraining*(Ytraining - Dtraining)*(tanh(Wout*Xtraining)'); %Calculate the gradient for the output layer
grad_w = 2/numTraining*(((Vout')*(Ytraining - Dtraining)).*(1-tanh(Wout*Xtraining).^2))*(Xtraining'); %..and for the hidden layer.



Wout = Wout - learningRate * grad_w; %Take the learning step.
Vout = Vout - learningRate * grad_v; %Take the learning step.

Ytraining = runMultiLayer(Xtraining, Wout, Vout);
Ytest = runMultiLayer(Xtest, Wout, Vout);

trainingError(1+n) = sum(sum((Ytraining - Dtraining).^2))/numTraining;
testError(1+n) = sum(sum((Ytest - Dtest).^2))/numTest;
end

end

